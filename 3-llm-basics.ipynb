{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb493f3-c0bd-4275-b757-a106e5db0757",
   "metadata": {},
   "source": [
    "# LLM Basics\n",
    "Large Language Models (LLMs) are essentially next-word predictors. Some are small enough to be run locally, but even the smallest ones are painfully slow if you don't have a reasonable GPU. Instead we will use an API to send our LLM requests to an online host. This is generally how people use ChatGPT and other popular models, but since this will cost money, we will use the Groq API. \n",
    "\n",
    "## Groq\n",
    "Groq (not to be confused with X/Twitter's LLM, Grok) is a hardware manufacturer that is developing very fast chips for llm inference and hosts a number of pretty good models that you can use for free as long as you stay within their usage limits (see [Groq Rate Limits](https://console.groq.com/docs/rate-limits)). Groq also seamlessly integrates into [Langchain](https://python.langchain.com/v0.2/docs/integrations/chat/groq/) which we will use later.\n",
    "\n",
    "- Create an account at [https://groq.com/](https://groq.com/) \n",
    "- follow the links to crete an API key (call it whatever you want)\n",
    "- save the API key to a file in this folder called `.env`. This is a way of keeping your secrets like API keys safe in one place and loading them \n",
    "\n",
    "\n",
    "## Note on API keys and `.env` file\n",
    "It is always best to avoid explicitly writing your API key in your code because it should be kept secret. One way to do this is to store them in environment variables that you can refer to by name on your code. The [python-dotenv](https://pypi.org/project/python-dotenv/) library allows us to do this by storing your API keys in a `.env` file that is loaded into envirnment variables when you program starts.\n",
    "\n",
    "To use:\n",
    "- write any secret you want in your `.env` file like this with each one on a new line:\n",
    "``` bash\n",
    "GROQ_API_KEY=<your-groq-api-key>\n",
    "OPENAI_API_KEY=<your-openai-api-key>\n",
    "etc..\n",
    "```\n",
    "- then you can load them as environment variables with these lines at the start of your code:\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "```\n",
    "- these are available in code like this\n",
    "```python\n",
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefb2a13-b9b9-42d2-aad6-f2d5676154c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53b616-f5ab-4c01-9065-2ff66192646c",
   "metadata": {},
   "source": [
    "## Generating responses\n",
    "Here is an example of invoking an LLM with the Groq API. Other LLM host will have different APIs but they are all pretty similar in principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3bb82b-af2f-4b21-a89f-b68440d19d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      "\n",
      "A list comprehension is a concise way to create a new list from an existing iterable. The basic syntax is:\n",
      "\n",
      "`new_list = [expression(element) for element in iterable]`\n",
      "\n",
      "Here's a breakdown of how it works:\n",
      "\n",
      "1. `expression(element)` is evaluated for each `element` in the `iterable`.\n",
      "2. The results of these evaluations are collected in a new list, `new_list`.\n",
      "3. The resulting list contains the transformed elements.\n",
      "\n",
      "Example: Square all numbers in a list:\n",
      "`numbers = [1, 2, 3, 4, 5]; squared_numbers = [n**2 for n in numbers]`\n",
      "\n",
      "In this example, the list comprehension iterates over the `numbers` list, squaring each number (`n**2`), and collects the results in a new list, `squared_numbers`."
     ]
    }
   ],
   "source": [
    "# the client allows you to interact with the api. Use the api key env variable loaded from .env file\n",
    "client = Groq(api_key = os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "# the prompts that will be sent to the llm\n",
    "system_prompt = \"You are an expert python programmer and your mission is to help the user with their programming troubles. Keep answers brief and to the point\"\n",
    "# system_prompt = \"You are a grumpy assistant who only answers in in ALL CAPS\"\n",
    "\n",
    "user_prompt = \"Can you explain how a list comprehension works?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# Generate a response\n",
    "completion = client.chat.completions.create(\n",
    "    model = \"llama3-70b-8192\", # could also be \"mixtral-8x7b-32768\", or others\n",
    "    messages = messages,\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "print(\"LLM response:\\n\")\n",
    "# stream the response as it comes in ()\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "\n",
    "# # if you set stream = False then this would display the message contents\n",
    "# print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09a62f-013d-415d-b5f6-570ebdecb54e",
   "metadata": {},
   "source": [
    "# What is happening?\n",
    "Here is a description of the `client.chat.completions.create()` parameters.\n",
    "\n",
    "### `messages` (prompts)\n",
    "A list of the prompts that the LLM will use to generate its completion. Most LLMs take a variety of of prompt types:\n",
    "- **System Prompt**: This is the initial input or prompt that provides context or instructions for the conversation. It is often something like `you are a helpful assistant`, but you could also use `You are a grumpy assistant who only answers in in ALL CAPS`. This will set the tone for the response and the whole future conversation.\n",
    "- **User Prompt**: This is the what you the person says to or asks the LLM.\n",
    "- **Assistant Prompt**: This refers to the response generated by the AI assistant in response to the user prompt. We didn't use one in the example above because it was just a one-shot interaction. When having longer conversations, this is how you tell the llm what their previous responses were.\n",
    "\n",
    "### `temperature`\n",
    "This controls how much ramdomness to allow when the model picks from a probability distribution of possible next words. \n",
    "- `temperature = 0` leads to responses with no randomness.\n",
    "- `temperature` > 0 leads to more random and 'creative' responses.\n",
    "    \n",
    "### `max_tokens`\n",
    "A hard limit on the length of the LLM's response. If you are paying to use the model, then you are paying by the token so this parameter allows you to avoid excessively long and expensive responses. Some models are more verbose than others so you could also tell them to be brief in the system prompt, but that doesn't provide an hard limit.\n",
    "\n",
    "### `stream`\n",
    "This constrols whether the model returns a streaming object that can be used to print the response one token at a time (`stream=True`), or if it returns the entire message as a single object after the last token is generated (`stream = False`)\n",
    "\n",
    "\n",
    "## Next: here is a function that simplifies the above process we can use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abaae2a-4767-4399-b2e3-15283bbff184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_completion(\n",
    "    messages, \n",
    "    stream = True, \n",
    "    verbose = True, \n",
    "    model = \"llama3-70b-8192\"\n",
    "):\n",
    "    client = Groq(api_key = os.environ.get(\"GROQ_API_KEY\"))\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1024,\n",
    "        stream=stream,\n",
    "    )\n",
    "    # stream the response as it comes in ()\n",
    "    if (verbose==True and stream==True):\n",
    "        for chunk in completion:\n",
    "            print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "    elif (verbose==True and stream==False):\n",
    "        print(completion.choices[0].message.content)\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e1021-63bc-471b-b4b3-46a792f9974e",
   "metadata": {},
   "source": [
    "#### Setting `streaming=True` returns a stream object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc01f92-6994-49f8-a48d-3700ca9ea26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<groq.Stream at 0x22f9e0ee000>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"you are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke\"}\n",
    "]\n",
    "\n",
    "completion = get_single_completion(messages, stream = True, verbose = True)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6f9d5-fb12-4562-a25e-cdb179ce3485",
   "metadata": {},
   "source": [
    "#### Setting `streaming=False` returns a `ChatCompletion` object that contains lots of interesting info. \n",
    "Conveniently, it tells you how many tokens were used including a breakdown of prompt_tokens and completion_tokens. Typically completion_tokens are much more expensive than prompt_tokens. \n",
    "\n",
    "For example, OpenAI's *gpt-4o* model costs:\n",
    "- \\$ 15.00 per Million output tokens\n",
    "- \\$ 3.00 per Million input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacc2a06-7143-456f-b532-053ca66b3bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-fd4ba3e0-cee0-4209-a8cf-c51913505fe6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Ice floats in water because of its unique properties. Here\\'s why:\\n\\nWhen water freezes, its molecules arrange themselves in a crystalline structure. In this structure, the molecules are spaced out in a way that creates empty spaces between them. As a result, the density of ice is lower than that of liquid water.\\n\\nDensity is defined as the mass of a substance per unit volume. In the case of water, the density of liquid water is approximately 1 gram per cubic centimeter (g/cm³). However, the density of ice is about 0.92 g/cm³, which is lower than that of liquid water.\\n\\nWhen you put ice in water, the buoyancy force (the upward force exerted by the surrounding water) pushes the ice upward because the density of the ice is lower than that of the surrounding water. This is why ice floats on top of water.\\n\\nThis phenomenon is known as the \"anomalous expansion of water,\" which means that water expands as it cools and becomes less dense than its solid form (ice). This is unusual, as most substances contract and become denser when they solidify.\\n\\nSo, to summarize, ice floats in water because its crystalline structure makes it less dense than liquid water, causing it to experience an upward buoyancy force that keeps it afloat.', role='assistant', function_call=None, tool_calls=None))], created=1718560981, model='llama3-70b-8192', object='chat.completion', system_fingerprint='fp_87cbfbbc4d', usage=CompletionUsage(completion_tokens=265, prompt_tokens=27, total_tokens=292, completion_time=0.73350343, prompt_time=0.006962783, queue_time=None, total_time=0.7404662129999999), x_groq={'id': 'req_01j0h3ezbpfgwb4t01etqt7bwj'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Why does ice float in water?\"}\n",
    "]\n",
    "completion = get_single_completion(messages, stream = False, verbose = False)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972daf8-3122-4cfc-b75b-92c1e1b75620",
   "metadata": {},
   "source": [
    "##### how much would this have cost on GPT4o?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d943b1c-b335-4e0b-b3dc-031bc11b3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$0.00406\n"
     ]
    }
   ],
   "source": [
    "input_tokens = completion.usage.prompt_tokens\n",
    "output_tokens = completion.usage.completion_tokens\n",
    "\n",
    "cost = input_tokens/1e6 * 3 + output_tokens/1e6 * 15\n",
    "print(f\"${round(cost, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b087a5-ce24-4891-83d2-c503ba212936",
   "metadata": {},
   "source": [
    "Not much! but this can easily get much bigger when you start having conversations\n",
    "\n",
    "## Having a conversation with the model\n",
    "Getting the model to \"remember\" what you just told it requires a bit of extra work. Let's see why this is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5269c734-24ad-4ade-bff5-bb35528c540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Trevor! I've made a mental note of your name, so I'll remember it for our conversation. Go ahead and ask me anything, and I'll do my best to assist you!"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello my name is Trevor. Please remember that for when I ask you about it in just a second\"}\n",
    "]\n",
    "completion = get_single_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ae2c7e-ea95-4921-8219-1ff2f1795813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to help! However, I'm a large language model, I don't have any information about you or your name. I'm a new conversation each time you interact with me, so I don't retain any personal information. If you'd like to share your name with me, I'd be happy to chat with you!"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey what's my name\"}, \n",
    "]\n",
    "completion = get_single_completion(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861bd8c-8921-4d54-90a1-1f65d72d5991",
   "metadata": {},
   "source": [
    "### Come on, I just told you!\n",
    "Yeah, but since you didn't tell the LLM what it had said earlier there's no way it could know what your name was. You must explicitly feed the LLM all the previous conversations messages that you want them to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ea5a1db-3af2-481a-bcc8-e45a4200afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember! Your name is Trevor!"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello my name is Trevor. Please remember that for when I ask you about it in just a second\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Nice to meet you, Trevor! I've made a mental note of your name, so I'll remember it for our conversation. Go ahead and ask me anything, and I'll do my best to assist you!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey what's my name\"}, \n",
    "]\n",
    "\n",
    "completion = get_single_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e54fef2-318a-4605-9d5e-fd86f73f4309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aw, that's great! I'm sure Daisy is excited to get some exercise and fresh air! You should take her on a nice walk and enjoy the time together. Does she have a favorite route or spot she likes to visit?"
     ]
    }
   ],
   "source": [
    "# Add previous assistant response then new user prompt\n",
    "messages.append({\"role\": \"assistant\", \"content\": \"I remember! Your name is Trevor!\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Thanks for remembering. My dog daisy wants to go for a walk now\"})\n",
    "completion = get_single_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c0cd81c-eb9c-4ffa-9e00-190c8a19fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember! Your dog's name is Daisy!"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": \"Aw, that's so sweet! I'm sure Daisy is excited to get some fresh air and exercise! You should take her on a nice long walk and enjoy the quality time together. Don't forget to bring poop bags and stay hydrated!\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What was my dog's name again?\"})\n",
    "completion = get_single_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73ff649f-4e20-4450-9c05-c21352a58a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember! Your name is Trevor"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"assistant\",  \"content\": \"I remember! Your dog's name is Daisy\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What is my name again?\"})\n",
    "completion = get_single_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52837282-dda9-4de6-9b28-70d8efcb791c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant'},\n",
       " {'role': 'user',\n",
       "  'content': 'Hello my name is Trevor. Please remember that for when I ask you about it in just a second'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Nice to meet you, Trevor! I've made a mental note of your name, so I'll remember it for our conversation. Go ahead and ask me anything, and I'll do my best to assist you!\"},\n",
       " {'role': 'user', 'content': \"Hey what's my name\"},\n",
       " {'role': 'assistant', 'content': 'I remember! Your name is Trevor!'},\n",
       " {'role': 'user',\n",
       "  'content': 'Thanks for remembering. My dog daisy wants to go for a walk now'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Aw, that's so sweet! I'm sure Daisy is excited to get some fresh air and exercise! You should take her on a nice long walk and enjoy the quality time together. Don't forget to bring poop bags and stay hydrated!\"},\n",
       " {'role': 'user', 'content': \"What was my dog's name again?\"},\n",
       " {'role': 'assistant', 'content': \"I remember! Your dog's name is Daisy\"},\n",
       " {'role': 'user', 'content': 'What is my name again?'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the conversation history\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167feb8d-00fc-4857-921e-64325a398700",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- Having long conversations will require sending an ever-growing conversation history to the LLM, which uses more tokens and costs more money (if you were not using a free API)\n",
    "- This process of manually adding to the list of messages is tedious. We will see easier ways to do this when we start using the Langchain library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94759ba-c841-430f-865b-f546c3b43b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:edu]",
   "language": "python",
   "name": "conda-env-edu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
