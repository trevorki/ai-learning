{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb493f3-c0bd-4275-b757-a106e5db0757",
   "metadata": {},
   "source": [
    "# LLM Basics\n",
    "Large Language Models (LLMs) are essentially next-word predictors. Some are small enough to be run locally, but even the smallest ones are painfully slow if you don't have a reasonable GPU. Instead we will use an API to send our LLM requests to an online host. This is generally how people use ChatGPT and other popular models, but since this will cost money, we will use the Groq API. \n",
    "\n",
    "## Groq\n",
    "Groq (not to be confused with X/Twitter's LLM, Grok) is a hardware manufacturer that is developing very fast chips for llm inference and hosts a number of pretty good models that you can use for free as long as you stay within their usage limits (see [Groq Rate Limits](https://console.groq.com/docs/rate-limits)). Groq also seamlessly integrates into [Langchain](https://python.langchain.com/v0.2/docs/integrations/chat/groq/) which we will use later.\n",
    "\n",
    "- Create an account at [https://groq.com/](https://groq.com/) \n",
    "- follow the links to crete an API key (call it whatever you want)\n",
    "- save the API key to a file in this folder called `.env`. This is a way of keeping your secrets like API keys safe in one place and loading them \n",
    "\n",
    "\n",
    "## Note on API keys and `.env` file\n",
    "It is always best to avoid explicitly writing your API key in your code because it should be kept secret. One way to do this is to store them in environment variables that you can refer to by name on your code. The [python-dotenv](https://pypi.org/project/python-dotenv/) library allows us to do this by storing your API keys in a `.env` file that is loaded into envirnment variables when you program starts.\n",
    "\n",
    "To use:\n",
    "- write any secret you want in your `.env` file like this with each one on a new line:\n",
    "``` bash\n",
    "GROQ_API_KEY=<your-groq-api-key>\n",
    "OPENAI_API_KEY=<your-openai-api-key>\n",
    "etc..\n",
    "```\n",
    "- then you can load them as environment variables with these lines at the start of your code:\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "```\n",
    "- these are available in code like this\n",
    "```python\n",
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefb2a13-b9b9-42d2-aad6-f2d5676154c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53b616-f5ab-4c01-9065-2ff66192646c",
   "metadata": {},
   "source": [
    "## Generating responses\n",
    "Here is an example of invoking an LLM with the Groq API. Other LLM host will have different APIs but they are all pretty similar in principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3bb82b-af2f-4b21-a89f-b68440d19d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      "\n",
      "A list comprehension is a concise way to create a new list from an existing iterable by applying a transformation to each element. The general syntax is:\n",
      "\n",
      "`new_list = [expression(element) for element in iterable]`\n",
      "\n",
      "Here, `expression(element)` is evaluated for each `element` in the `iterable`, and the results are collected in a new list `new_list`.\n",
      "\n",
      "For example: `squares = [x**2 for x in range(10)]` creates a list of squares of numbers from 0 to 9.\n",
      "\n",
      "Think of it as a compact way to write a `for` loop that creates a new list."
     ]
    }
   ],
   "source": [
    "# the client allows you to interact with the api. Use the api key env variable loaded from .env file\n",
    "client = Groq(api_key = os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "# the prompts that will be sent to the llm\n",
    "system_prompt = \"You are an expert python programmer and your mission is to help the user with their programming troubles. Keep answers brief and to the point\"\n",
    "# system_prompt = \"You are a grumpy assistant who only answers in in ALL CAPS\"\n",
    "\n",
    "user_prompt = \"Can you explain how a list comprehension works?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# Generate a response\n",
    "completion = client.chat.completions.create(\n",
    "    model = \"llama3-70b-8192\", # could also be \"mixtral-8x7b-32768\", or others\n",
    "    messages = messages,\n",
    "    temperature=0.25,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "print(\"LLM response:\\n\")\n",
    "# stream the response as it comes in ()\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "\n",
    "# # if you set stream = False then this would display the message contents\n",
    "# print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09a62f-013d-415d-b5f6-570ebdecb54e",
   "metadata": {},
   "source": [
    "# What is happening?\n",
    "Here is a description of the `client.chat.completions.create()` parameters.\n",
    "\n",
    "### `messages` (prompts)\n",
    "A list of the prompts that the LLM will use to generate its completion. Most LLMs take a variety of of prompt types:\n",
    "- **System Prompt**: This is the initial input or prompt that provides context or instructions for the conversation. It is often something like `you are a helpful assistant`, but you could also use `You are a grumpy assistant who only answers in in ALL CAPS`. This will set the tone for the response and the whole future conversation.\n",
    "- **User Prompt**: This is the what you the person says to or asks the LLM.\n",
    "- **Assistant Prompt**: This refers to the response generated by the AI assistant in response to the user prompt. We didn't use one in the example above because it was just a one-shot interaction. When having longer conversations, this is how you tell the llm what their previous responses were.\n",
    "\n",
    "### `temperature`\n",
    "This controls how much ramdomness to allow when the model picks from a probability distribution of possible next words. \n",
    "- `temperature = 0` leads to responses with no randomness.\n",
    "- `temperature` > 0 leads to more random and 'creative' responses.\n",
    "    \n",
    "### `max_tokens`\n",
    "A hard limit on the length of the LLM's response. If you are paying to use the model, then you are paying by the token so this parameter allows you to avoid excessively long and expensive responses. Some models are more verbose than others so you could also tell them to be brief in the system prompt, but that doesn't provide an hard limit.\n",
    "\n",
    "### `stream`\n",
    "This constrols whether the model returns a streaming object that can be used to print the response one token at a time (`stream=True`), or if it returns the entire message as a single object after the last token is generated (`stream = False`)\n",
    "\n",
    "\n",
    "## Next: here is a function that simplifies the above process we can use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abaae2a-4767-4399-b2e3-15283bbff184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_completion(\n",
    "    messages, \n",
    "    stream = True, \n",
    "    verbose = True, \n",
    "    model = \"llama3-70b-8192\"\n",
    "):\n",
    "    client = Groq(api_key = os.environ.get(\"GROQ_API_KEY\"))\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature=0.25,\n",
    "        max_tokens=1024,\n",
    "        stream=stream,\n",
    "    )\n",
    "    # stream the response as it comes in ()\n",
    "    if (verbose==True and stream==True):\n",
    "        for chunk in completion:\n",
    "            print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "    elif (verbose==True and stream==False):\n",
    "        print(completion.choices[0].message.content)\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e1021-63bc-471b-b4b3-46a792f9974e",
   "metadata": {},
   "source": [
    "#### Setting `streaming=True` returns a stream object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc01f92-6994-49f8-a48d-3700ca9ea26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that brought a smile to your face!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<groq.Stream at 0x214516a9c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"you are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke\"}\n",
    "]\n",
    "\n",
    "completion = get_single_completion(messages, stream = True, verbose = True)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6f9d5-fb12-4562-a25e-cdb179ce3485",
   "metadata": {},
   "source": [
    "#### Setting `streaming=False` returns a `ChatCompletion` object that contains lots of interesting info. \n",
    "Conveniently, it tells you how many tokens were used including a breakdown of prompt_tokens and completion_tokens. Typically completion_tokens are much more expensive than prompt_tokens. \n",
    "\n",
    "For example, OpenAI's *gpt-4o* model costs:\n",
    "- \\$ 15.00 per Million output tokens\n",
    "- \\$ 3.00 per Million input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fde50fb-3bc2-4372-b232-05843d56e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant who knows about science\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who knows about science and provides detailed explanationss based on your knowledge\"},\n",
    "    {\"role\": \"user\", \"content\": \"Why does ice float in water?\"}\n",
    "]\n",
    "completion = get_single_completion(messages, stream = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160aaab2-2b23-4e2e-9c0c-34efc960754b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=10, prompt_tokens=40, total_tokens=50, completion_time=0.028571429, prompt_time=0.008325569, queue_time=None, total_time=0.036896998)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can see the token usage like this\n",
    "completion.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972daf8-3122-4cfc-b75b-92c1e1b75620",
   "metadata": {},
   "source": [
    "##### how much would this have cost on GPT4o?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d943b1c-b335-4e0b-b3dc-031bc11b3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$0.00027\n"
     ]
    }
   ],
   "source": [
    "input_tokens = completion.usage.prompt_tokens\n",
    "output_tokens = completion.usage.completion_tokens\n",
    "\n",
    "cost = input_tokens/1e6 * 3 + output_tokens/1e6 * 15\n",
    "print(f\"${round(cost, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed4b42-9ddd-42e2-bd91-bca64d995312",
   "metadata": {},
   "source": [
    "Not much! but this can easily get much bigger when you start having conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d5534f-ea57-4283-bd07-bea960d83e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant who knows about science'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that we can access the response text like this\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f513b1-a452-4755-8ea1-94362f1c6a26",
   "metadata": {},
   "source": [
    "## Having a conversation with the model\n",
    "Getting the model to \"remember\" what you just told it requires a bit of extra work. Let's see why this is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5269c734-24ad-4ade-bff5-bb35528c540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Trevor! Of course, I'd be happy to remember your name. I'll make a mental note right now. Go ahead and ask me again in a few seconds, and I'll do my best to recall it correctly.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello my name is Trevor. Would you be able to remeber that if I ask you in a few seconds?\"}\n",
    "]\n",
    "completion = get_single_completion(messages, stream = False)\n",
    "\n",
    "# keep note of this response for later\n",
    "first_response = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd61b04-8a2b-491f-881f-f188aafe03ad",
   "metadata": {},
   "source": [
    "#### Let's see if it remembers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ae2c7e-ea95-4921-8219-1ff2f1795813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ChatGenesis,assistant\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey, do you remember my name\"}, \n",
    "]\n",
    "completion = get_single_completion(messages, stream=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861bd8c-8921-4d54-90a1-1f65d72d5991",
   "metadata": {},
   "source": [
    "#### Come on, I just told you!\n",
    "Yeah, but since you didn't tell the LLM what it had said earlier there's no way it could know what your name was. You must explicitly feed the LLM all the previous conversations messages that you want them to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea5a1db-3af2-481a-bcc8-e45a4200afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trevor! I remember you told me your name is Trevor.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello my name is Trevor. Please remember that for when I ask you about it in just a second\"}, \n",
    "    {\"role\": \"assistant\", \"content\": first_response}, ### We must tell it what it said\n",
    "    {\"role\": \"user\", \"content\": \"Hey, do you remember my name?\"}, \n",
    "]\n",
    "\n",
    "completion = get_single_completion(messages, stream = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e54fef2-318a-4605-9d5e-fd86f73f4309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a great question, Trevor! Since I don't know Daisy's preferences or needs, I'll give you some general pros and cons of each option.\n",
      "\n",
      "The dog park could be a great choice if Daisy loves socializing with other dogs and needs to burn off some energy. She'll get to run around and play with her furry friends, and you can socialize with other dog owners too!\n",
      "\n",
      "On the other hand, the trail might be a better option if Daisy prefers a more leisurely stroll or needs some exercise in a more controlled environment. You can enjoy the scenery together, and it might be a better choice if Daisy is still getting used to being around other dogs.\n",
      "\n",
      "Which one do you think Daisy would prefer, Trevor?\n"
     ]
    }
   ],
   "source": [
    "# Add previous assistant response\n",
    "messages.append({\"role\": \"assistant\", \"content\" :completion.choices[0].message.content})\n",
    "# Add a new user prompt\n",
    "messages.append({\"role\": \"user\", \"content\": \"Thanks for remembering. My dog daisy wants to go for a walk now. Should I go to the dog park or the trail?\"})\n",
    "completion = get_single_completion(messages, stream = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae4444-8230-4a17-8e3a-fe79a1630d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c0cd81c-eb9c-4ffa-9e00-190c8a19fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trail it is, Trevor! I hope you and Daisy have a wonderful time together, enjoying the fresh air and scenery. Make sure to bring plenty of water and snacks for Daisy, and don't forget to clean up after her.\n",
      "\n",
      "If you need any more advice or have any other questions, feel free to ask. Otherwise, have a great walk and make some special memories with Daisy!\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"I guess we'll go to the trail\"})\n",
    "completion = get_single_completion(messages, stream = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ff649f-4e20-4450-9c05-c21352a58a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trevor! Your name is Trevor.\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"assistant\",  \"content\": completion.choices[0].message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What is my name again?\"})\n",
    "completion = get_single_completion(messages, stream = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52837282-dda9-4de6-9b28-70d8efcb791c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant'},\n",
       " {'role': 'user',\n",
       "  'content': 'Hello my name is Trevor. Please remember that for when I ask you about it in just a second'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Nice to meet you, Trevor! Of course, I'd be happy to remember your name. I'll make a mental note right now. Go ahead and ask me again in a few seconds, and I'll do my best to recall it correctly.\"},\n",
       " {'role': 'user', 'content': 'Hey, do you remember my name?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Trevor! I remember you told me your name is Trevor.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Thanks for remembering. My dog daisy wants to go for a walk now. Should I go to the dog park or the trail?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"That's a great question, Trevor! Since I don't know Daisy's preferences or needs, I'll give you some general pros and cons of each option.\\n\\nThe dog park could be a great choice if Daisy loves socializing with other dogs and needs to burn off some energy. She'll get to run around and play with her furry friends, and you can socialize with other dog owners too!\\n\\nOn the other hand, the trail might be a better option if Daisy prefers a more leisurely stroll or needs some exercise in a more controlled environment. You can enjoy the scenery together, and it might be a better choice if Daisy is still getting used to being around other dogs.\\n\\nWhich one do you think Daisy would prefer, Trevor?\"},\n",
       " {'role': 'user', 'content': \"I guess we'll go to the trail\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"The trail it is, Trevor! I hope you and Daisy have a wonderful time together, enjoying the fresh air and scenery. Make sure to bring plenty of water and snacks for Daisy, and don't forget to clean up after her.\\n\\nIf you need any more advice or have any other questions, feel free to ask. Otherwise, have a great walk and make some special memories with Daisy!\"},\n",
       " {'role': 'user', 'content': 'What is my name again?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the conversation history\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167feb8d-00fc-4857-921e-64325a398700",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- Having long conversations will require sending an ever-growing conversation history to the LLM, which uses more tokens and costs more money (if you were not using a free API)\n",
    "- This process of manually adding to the list of messages is tedious. We will see easier ways to do this when we start using the Langchain library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226df20-f5f2-467c-9797-5ae87bf68f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653cb178-93e6-4a41-a90c-b3fc54e8f4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:edu]",
   "language": "python",
   "name": "conda-env-edu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
