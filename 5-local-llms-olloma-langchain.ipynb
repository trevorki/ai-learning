{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15648e6a-9134-44f0-80e1-2f7be8f400cb",
   "metadata": {},
   "source": [
    "# Local LLMs with Ollama and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefb2a13-b9b9-42d2-aad6-f2d5676154c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf077b-3455-4dcd-b2e8-8d1bf902f449",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "Ollama is a program that allows you to download, run, and interaact with LLMs on your local machine using the terminal.\n",
    "\n",
    "**[Install Ollama from here](https://ollama.com/download/windows)**\n",
    "\n",
    "Once Ollama is installed open a terminal and enter:\n",
    "```bash\n",
    "$ ollama run phi3\n",
    "```\n",
    "This will download Phi3 (a 3 billion parameter model from Microdoft that is pretty good) if not already downloaded, then open a chat session. If you don't have a GPU you will see how slow these models run, even if they are relatively small. For comparison GPT-4 has 1.76 TRILLION parameters!\n",
    "\n",
    "You can use Ollama with many open-source models, like Llama3, Mistral, and Gemma.\n",
    "\n",
    "![Ollama screenshot](img/ollama-screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4e147-3c96-429b-bfcf-7feb52768ee4",
   "metadata": {},
   "source": [
    "# Using Ollama in Langchain\n",
    "Lanchain has an Ollama integration allowing you to use your local models as the LLM in any langchain process. It uses the models you have already downloaded on your own machine, so no API key is equired.\n",
    "\n",
    "This is very nice feature of Langchain, allowing you to easily swap out a remote model for a local one without having to alter your code very much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a10f69e-8677-4d6f-b7f5-7b18ed621bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"phi3\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609352ce-484c-4543-90ba-dab1b45ea2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ice floats on water because it is less dense than liquid water. When water freezes, its molecules form a crystalline structure that occupies more space, making the solid state of water (ice) lighter per unit volume compared to its liquid counterpart. This lower density causes ice to float.\n"
     ]
    }
   ],
   "source": [
    "# you can invoke it like any langchain model\n",
    "prompt = \"Why does ice float in water? Be very brief\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce79a1-2e7d-455f-b29c-b1d38e4b0ea9",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "You can strem the resutls in langchain as shown below. This allows you to see each token as it is generated. It's pretty slow, but streaming is nice because it communicates to the user that the system is working and not frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20423043-abb7-4921-be22-9259ce638201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ice floats on water because it is less dense than liquid water. When water freezes, its molecules form a crystalline structure that occupies more space, making the solid state of water (ice) lighter per unit volume compared to its liquid counterpart. This lower density causes ice to float."
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in llm.astream(prompt):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bd5d1-50b0-477d-8618-c6a8b06e06ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:edu]",
   "language": "python",
   "name": "conda-env-edu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
