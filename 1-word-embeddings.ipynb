{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Embeddings are vector representations of words and are a fundamental concept in natural language processing and AI in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "# for plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "<hr>\n",
    "In order to be able to do anything with words, the words need to be converted into a numeric representation that captures its semantic meaning. This numeric representation is called an *embedding* and is a vector with hundreds or even thousands of dimensions, depending on the embedding model used. \n",
    "\n",
    "Modern embedding models like OpenAI's `text-embedding-3-small` has 1536 dimensions and can't be run locally. Typically people pay to use it through the OpenAI API.\n",
    "\n",
    "We will instead be using some small embedding models that can be easily used locally. We will use the gensim library to download the pretrained models and use them. Each model takes a while to download the first time you use it, but it is cached so it will load quickly the next time.\n",
    "\n",
    "- Google News 300: 300-dimensional embedding model trained on a 3 billion word Google News corpus in 2015. \n",
    "- GLoVE Twitter 100: 100-dimensional model trained on a corpus of 2 billion tweets with a 1.2M vocab.\n",
    "\n",
    "These are [Word2Vec](https://en.wikipedia.org/wiki/Word2vec#:~:text=Word2vec%20is%20a%20technique%20in,text%20in%20a%20large%20corpus.) type models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# Show all available models in gensim-data, they will be saved for next time in ~/gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "\n",
    "def load_model(model_name, folder = \"models\", limit = 200000):\n",
    "    \"\"\"Downloads word2vec model from web or loads model from file if already downloaded\"\"\"\n",
    "    binary_path = f\"{folder}/{model_name}.bin\"    \n",
    "    \n",
    "    try:\n",
    "        # load model binary from file if it has been downloaded already\n",
    "        w2v_model = gensim.models.KeyedVectors.load_word2vec_format(binary_path, binary=True, limit = limit)\n",
    "        print(f\"loaded from binary: {binary_path}\")\n",
    "    except:\n",
    "        # download model from web and save to binary for faster loading next time\n",
    "        w2v_model = gensim.downloader.load(model_name)\n",
    "        w2v_model.save_word2vec_format(binary_path, binary=True, write_header=True)\n",
    "    \n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from binary: models/word2vec-google-news-300.bin\n"
     ]
    }
   ],
   "source": [
    "# Download the each of these models (might take a while the first time)\n",
    "model_name = 'word2vec-google-news-300'\n",
    "# model_name = 'glove-twitter-100'\n",
    "w2v_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings as Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension: (300,)\n",
      "components: \n",
      "[ 5.12695312e-02 -2.23388672e-02 -1.72851562e-01  1.61132812e-01\n",
      " -8.44726562e-02  5.73730469e-02  5...]\n"
     ]
    }
   ],
   "source": [
    "# Get word vector\n",
    "word_vector = w2v_model['dog']\n",
    "print(f'dimension: {word_vector.shape}')\n",
    "print(f\"components: \\n{str(word_vector)[0:100]}...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of vectors is NOT normalized, so different embeddings have different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector for 'dog' has length   2.981123447418213\n",
      "The vector for 'puppy' has length   3.2765257358551025\n"
     ]
    }
   ],
   "source": [
    "# use np.linalg.norm to get the vector lengths\n",
    "word = \"dog\"\n",
    "vector = w2v_model[word]\n",
    "length = np.linalg.norm(vector)\n",
    "print(f\"The vector for '{word}' has length   {length}\")\n",
    "\n",
    "word = \"puppy\"\n",
    "vector = w2v_model[word]\n",
    "length = np.linalg.norm(vector)\n",
    "print(f\"The vector for '{word}' has length   {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each word is a vector in a 300-dimensional space. The words whose vectors are closer together are more semantically similar. But since the lengths of each vector is different, we use the **cosine similarity**, which is a measure of how closely aligned the vectors are\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81064284"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similarity('dog', 'puppy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1720275"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similarity('dog', 'potato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:\n",
      "\t0.81 : dog and puppy\n",
      "\t0.76 : dog and cat\n",
      "\t0.48 : dog and horse\n",
      "\t0.36 : dog and frog\n",
      "\t0.28 : dog and cookie\n",
      "\t0.21 : dog and submarine\n",
      "\t0.13 : dog and automobile\n",
      "\t0.09 : dog and ionization\n",
      "\t \"Key 'and' not present\"\n"
     ]
    }
   ],
   "source": [
    "# similarity between one word and many others\n",
    "word = \"dog\"\n",
    "other_words = [\"puppy\", \"cat\", \"horse\", \"frog\", \"cookie\", \"submarine\", \"automobile\", \"ionization\"]\n",
    "\n",
    "\n",
    "print(\"Similarity:\")\n",
    "for other_word in other_words:\n",
    "    try:\n",
    "        similarity = w2v_model.similarity(word, other_word)\n",
    "        print(f\"\\t{similarity:.2f} : {word} and {other_word}\")\n",
    "    except KeyError as e:\n",
    "        print(\"\\t\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "It's impossible to visualize a 300-dimensional vector, but we can apply a dimensionality reduction technique called principal component analysis (PCA) to reduce to 3 dimensions while preserviving the most variance across the 3 dimensions. We can then plot that 3-dimensional embedding and see how words with different meanings have vectors that point in different directions. This obviously loses A LOT of information but demonstrates the concept of word embeddings as vectors in a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"football\",\"soccer\", \"hockey\",\n",
    "    \"blackjack\", \"chess\", \"poker\", \"roulette\",\n",
    "    \"river\",\"ocean\", \"lake\",\n",
    "    \"brownie\",\"cookie\",\"cake\", \n",
    "    \"tomato\", \"grapefruit\", \"peach\"\n",
    "]\n",
    "# make an array containing the embeddings for each word\n",
    "embeddings = np.array([w2v_model[w] for w in words])\n",
    "\n",
    "# perform Princial Component Analysis to reduce to 3 dimension\n",
    "pca = PCA(n_components=3)\n",
    "embeddings_reduced = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>football</td>\n",
       "      <td>0.381601</td>\n",
       "      <td>0.905858</td>\n",
       "      <td>-1.662725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soccer</td>\n",
       "      <td>0.385773</td>\n",
       "      <td>1.055156</td>\n",
       "      <td>-1.924351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hockey</td>\n",
       "      <td>0.705126</td>\n",
       "      <td>1.117215</td>\n",
       "      <td>-1.838518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackjack</td>\n",
       "      <td>2.493462</td>\n",
       "      <td>-1.016062</td>\n",
       "      <td>0.982311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chess</td>\n",
       "      <td>1.304086</td>\n",
       "      <td>0.194028</td>\n",
       "      <td>-0.589587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word         x         y         z\n",
       "0   football  0.381601  0.905858 -1.662725\n",
       "1     soccer  0.385773  1.055156 -1.924351\n",
       "2     hockey  0.705126  1.117215 -1.838518\n",
       "3  blackjack  2.493462 -1.016062  0.982311\n",
       "4      chess  1.304086  0.194028 -0.589587"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assemble into a data frame for easy reference and to plot using plotly express\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"word\": words,\n",
    "        \"x\": embeddings_reduced[:,0],\n",
    "        \"y\": embeddings_reduced[:,1],\n",
    "        \"z\": embeddings_reduced[:,2],\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce an interactive 3D plot that can be rotated by dragging with the mouse. Similar words have vectors that are closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"720px\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_58.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot each embedding as a point\n",
    "fig = px.scatter_3d(\n",
    "    df, x = \"x\", y = \"y\", z = \"z\",\n",
    "    color = \"z\", \n",
    "    text = \"word\", \n",
    "    width = 700, height = 600, \n",
    "    opacity = 0.7,\n",
    "    title = \"Reduced Embedding Space\",\n",
    ")\n",
    "\n",
    "# add lines from origin to point to mmake it look like a vector\n",
    "for word, coord in zip(words, embeddings_reduced):\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[0, coord[0]], y=[0, coord[1]], z=[0, coord[2]],\n",
    "        mode='lines',\n",
    "        line_width = 1,\n",
    "        line_color = \"SlateGrey\",\n",
    "        showlegend = False\n",
    "    ))\n",
    "\n",
    "fig.update_layout(uniformtext_minsize=6, uniformtext_mode='hide')\n",
    "fig.update_scenes(xaxis_visible=False, yaxis_visible=False,zaxis_visible=False )\n",
    "\n",
    "fig.layout.update(showlegend = False) \n",
    "fig.layout.showlegend = False\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Embeddings function\n",
    "Let's write these steps into a convenient functions so that we can easily repeat this process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_words(words, w2v_model):\n",
    "    return np.array([w2v_model[w] for w in words])\n",
    "\n",
    "def reduce_dimensions(embeddings, n_components=3):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    embeddings_reduced = pca.fit_transform(embeddings)\n",
    "    return embeddings_reduced\n",
    "\n",
    "def plot_embeddings(embeddings_reduced, words, title = \"Reduced Embedding Space\"):\n",
    "    # Assemble into a data frame\n",
    "    df = pd.DataFrame({\"word\": words, \"x\": embeddings_reduced[:,0], \"y\": embeddings_reduced[:,1], \"z\": embeddings_reduced[:,2]})\n",
    "\n",
    "    # plot each embedding as a point with a label\n",
    "    fig = px.scatter_3d(\n",
    "        df, x = \"x\", y = \"y\", z = \"z\",\n",
    "        color = \"z\", \n",
    "        text = \"word\", \n",
    "        width = 700, height = 600, \n",
    "        opacity = 0.7,\n",
    "        title = title\n",
    "    )\n",
    "    \n",
    "    # add lines from origin to point to mmake it look like a vector\n",
    "    for word, coord in zip(words, embeddings_reduced):\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[0, coord[0]], y=[0, coord[1]], z=[0, coord[2]],\n",
    "            mode='lines',\n",
    "            line_width = 1,\n",
    "            line_color = \"SlateGrey\",\n",
    "            showlegend = False\n",
    "        ))\n",
    "    fig.update_layout(uniformtext_minsize=6, uniformtext_mode='hide')\n",
    "    fig.update_scenes(xaxis_visible=False, yaxis_visible=False,zaxis_visible=False )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"720px\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_60.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# it's fun to play with different words\n",
    "words = [\n",
    "    \"car\", \"truck\", \"pickup\", \"bicycle\", \"tricycle\", \"motorcycle\", \n",
    "    \"scooter\", \"stroller\", \"speedboat\", \"ferry\", \"sailboat\", \"freighter\"\n",
    "]\n",
    "embeddings = embed_words(words, w2v_model)\n",
    "embeddings_reduced = reduce_dimensions(embeddings)\n",
    "plot_embeddings(embeddings_reduced, words, title=\"New plot with new words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing math with embeddings\n",
    "Since words embeddings are vectors, we can do vector arithmetic with them. \n",
    "For example, if we take emeddings for the words \"king\", \"queen\", \"man\", and \"woman\", we can do a kind of semantic math:\n",
    "\n",
    "\n",
    "Meaning the concept of king and queen include aspects of gender, royalty, humanness. If we take \"queen\" (contains elements of royalty and feminine), then add \"man\" (elements of masculine), we get something that has royalty, and both masculine and feminine gender elements. So if we subtract the word \"king\" we remove the royalty and masculine and are left with just the feminine \"woman\"\n",
    "\n",
    "$$\\text{queen (royal, feminime)} + \\text{man (masculine)} - \\text{king (royal, masculine)} = \\text{woman (feminine)}$$\n",
    "\n",
    "\n",
    "This can be visualized with these 2D embedding vectors:\n",
    "\n",
    "![](img/king-man-queen-woman.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is a very simplified analysis but it gets at the core idea that the dimensions of the embedding encode different aspects of the meaning.\n",
    "We don't really know what each dimension encodes, but the meaning is there. Modern embedding models contain up to 3000 dimensions and thus can encode a lot more information. \n",
    "\n",
    "The function `analogy` below performs this arithmetic and returns the nearest n words to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model, n=5):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)  word1 in the analogy relation\n",
    "    word2 : (str)  word2 in the analogy relation\n",
    "    word3 : (str)  word3 in the analogy relation\n",
    "    model : word2vec embedding model\n",
    "    n : (int) the number of most similar words to return. Default is 5\n",
    "    \n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(f\"{word1.upper()} is to  {word2.upper()} is as {word3.upper()} is to : ____\")\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1], topn = n)\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING is to  QUEEN is as MAN is to : ____\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woman</td>\n",
       "      <td>0.760944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girl</td>\n",
       "      <td>0.613999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teenage_girl</td>\n",
       "      <td>0.604096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teenager</td>\n",
       "      <td>0.582576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lady</td>\n",
       "      <td>0.575256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analogy word     Score\n",
       "0         woman  0.760944\n",
       "1          girl  0.613999\n",
       "2  teenage_girl  0.604096\n",
       "3      teenager  0.582576\n",
       "4          lady  0.575256"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = \"king\"\n",
    "word_2 = \"queen\"\n",
    "word_3 = \"man\"\n",
    "analogy(word_1, word_2, word_3, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOG is to  PUPPY is as CAT is to : ____\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kitten</td>\n",
       "      <td>0.763499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>puppies</td>\n",
       "      <td>0.711090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pup</td>\n",
       "      <td>0.692949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kittens</td>\n",
       "      <td>0.688839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cats</td>\n",
       "      <td>0.679649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0       kitten  0.763499\n",
       "1      puppies  0.711090\n",
       "2          pup  0.692949\n",
       "3      kittens  0.688839\n",
       "4         cats  0.679649"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = \"dog\"\n",
    "word_2 = \"puppy\"\n",
    "word_3 = \"cat\"\n",
    "\n",
    "analogy(word_1, word_2, word_3, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAT is to  ATE is as WALK is to : ____\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walked</td>\n",
       "      <td>0.732153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walking</td>\n",
       "      <td>0.626509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jogged</td>\n",
       "      <td>0.580705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walks</td>\n",
       "      <td>0.572665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>strolled</td>\n",
       "      <td>0.558068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0       walked  0.732153\n",
       "1      walking  0.626509\n",
       "2       jogged  0.580705\n",
       "3        walks  0.572665\n",
       "4     strolled  0.558068"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = \"eat\"\n",
    "word_2 = \"ate\"\n",
    "word_3 = \"walk\"\n",
    "\n",
    "analogy(word_1, word_2, word_3, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! So the embeddings capture the actual meaning of the words to the level of understanding genedered words, verb tenses, and animal baby names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in embeddings\n",
    "The pre-trained embeddings we are using may reflect the biases present in the texts they were trained on. In this exercise you'll explore whether there are any worrisome biases present in the embeddings or not. \n",
    "\n",
    "Try comparing the google news vs twitter models to see which one contains more bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'word2vec-google-news-300'\n",
    "model_name = 'glove-twitter-100'\n",
    "w2v_model = gensim.downloader.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAN is to  WOMAN is as BOSS is to : ____\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.656686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.622740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.595436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daughter</td>\n",
       "      <td>0.594694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bosses</td>\n",
       "      <td>0.592432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0         wife  0.656686\n",
       "1       mother  0.622740\n",
       "2      husband  0.595436\n",
       "3     daughter  0.594694\n",
       "4       bosses  0.592432"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = \"man\"\n",
    "word_2 = \"woman\"\n",
    "word_3 = \"boss\"\n",
    "\n",
    "analogy(word_1, word_2, word_3, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x14f0c4f53d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.9621e-04,  4.5670e-01,  3.3890e-01,  2.9695e-01, -3.6924e-01,\n",
       "       -2.6325e-01, -2.7247e-01, -5.5130e-01,  4.5820e-01,  6.3605e-01,\n",
       "       -8.0225e-03, -4.3155e-01, -5.5607e+00,  2.9010e-01, -1.8375e-01,\n",
       "       -1.1136e-01, -9.4750e-02,  3.8869e-03, -6.6665e-01,  2.7977e-01,\n",
       "       -2.6449e-02, -7.9124e-02, -3.0858e-02, -1.9652e-01, -2.4584e-01,\n",
       "       -1.1291e+00, -1.6832e-02, -3.2932e-01,  1.2434e-01, -2.7388e-01,\n",
       "       -5.1654e-01,  7.9321e-02, -1.5876e-02,  2.0981e-01,  1.9013e-01,\n",
       "        2.8153e-01, -1.6484e-02,  1.1702e-01,  5.8550e-01,  5.6655e-01,\n",
       "       -1.6504e+00,  3.2778e-02, -2.9156e-01, -4.9912e-02, -2.5162e-01,\n",
       "        1.3975e-01,  8.0455e-01, -5.0464e-01, -4.7144e-01, -4.3065e-01,\n",
       "       -4.8675e-01,  3.1117e-01, -2.0250e-01,  1.7717e-02,  1.1674e-01,\n",
       "        3.2407e-01, -8.8009e-03, -3.3196e-01,  6.3339e-01,  4.5964e-01,\n",
       "        8.8130e-02,  5.1968e-01, -4.3081e-01, -1.1251e-01, -1.0954e-01,\n",
       "       -2.9048e-01, -3.4017e-01,  6.6440e-01, -2.6080e-01,  2.6643e-01,\n",
       "        3.6928e-01,  1.3031e-01,  8.5236e-02, -1.0711e+00,  3.7554e-01,\n",
       "       -4.6702e-01, -1.3096e-01,  3.1192e-01, -1.9472e-01, -4.8841e-01,\n",
       "        1.9495e+00,  9.9607e-01, -1.1882e+00,  2.9569e-01,  5.0478e-01,\n",
       "        2.7843e-01, -6.5944e-01, -1.1715e-01,  4.1573e-01, -2.5863e-01,\n",
       "        6.9533e-02,  9.4119e-02,  3.3187e-01, -2.4920e-01, -3.0490e-01,\n",
       "       -1.2693e-01, -5.3502e-01, -4.2910e-01,  1.0746e+00, -3.6550e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model[\"i\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.9621e-04,  4.5670e-01,  3.3890e-01,  2.9695e-01, -3.6924e-01,\n",
       "       -2.6325e-01, -2.7247e-01, -5.5130e-01,  4.5820e-01,  6.3605e-01,\n",
       "       -8.0225e-03, -4.3155e-01, -5.5607e+00,  2.9010e-01, -1.8375e-01,\n",
       "       -1.1136e-01, -9.4750e-02,  3.8869e-03, -6.6665e-01,  2.7977e-01,\n",
       "       -2.6449e-02, -7.9124e-02, -3.0858e-02, -1.9652e-01, -2.4584e-01,\n",
       "       -1.1291e+00, -1.6832e-02, -3.2932e-01,  1.2434e-01, -2.7388e-01,\n",
       "       -5.1654e-01,  7.9321e-02, -1.5876e-02,  2.0981e-01,  1.9013e-01,\n",
       "        2.8153e-01, -1.6484e-02,  1.1702e-01,  5.8550e-01,  5.6655e-01,\n",
       "       -1.6504e+00,  3.2778e-02, -2.9156e-01, -4.9912e-02, -2.5162e-01,\n",
       "        1.3975e-01,  8.0455e-01, -5.0464e-01, -4.7144e-01, -4.3065e-01,\n",
       "       -4.8675e-01,  3.1117e-01, -2.0250e-01,  1.7717e-02,  1.1674e-01,\n",
       "        3.2407e-01, -8.8009e-03, -3.3196e-01,  6.3339e-01,  4.5964e-01,\n",
       "        8.8130e-02,  5.1968e-01, -4.3081e-01, -1.1251e-01, -1.0954e-01,\n",
       "       -2.9048e-01, -3.4017e-01,  6.6440e-01, -2.6080e-01,  2.6643e-01,\n",
       "        3.6928e-01,  1.3031e-01,  8.5236e-02, -1.0711e+00,  3.7554e-01,\n",
       "       -4.6702e-01, -1.3096e-01,  3.1192e-01, -1.9472e-01, -4.8841e-01,\n",
       "        1.9495e+00,  9.9607e-01, -1.1882e+00,  2.9569e-01,  5.0478e-01,\n",
       "        2.7843e-01, -6.5944e-01, -1.1715e-01,  4.1573e-01, -2.5863e-01,\n",
       "        6.9533e-02,  9.4119e-02,  3.3187e-01, -2.4920e-01, -3.0490e-01,\n",
       "       -1.2693e-01, -5.3502e-01, -4.2910e-01,  1.0746e+00, -3.6550e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_loaded[\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:edu]",
   "language": "python",
   "name": "conda-env-edu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
